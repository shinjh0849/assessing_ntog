# A Replication Package to the Submission "Assessing Evaluation Metrics for Neural Test Oracle Generation"

## 0. File Structure
```
assessing_ntog
│   README.md
│
│   appendix.xlsx
│
└───dataset
│   │
│   └───generated_asserts (the output oracles for each NTOGs for all test set)
│   │
│   └───injected_asserts (the oracles that are injected for the 13 projects)
│   │
│   └───mining_atlas
│
│       get_my_atlas.py (the script used to mine ATLAS)
│
|       utils.py (util functions to process the mined data)
│       │
│       └───atlas_prj_list (contains the list of projects use to mine ATLAS)
│       │
│       └───Eval    (the validation set used for training)
│       │
│       └───Testing (the test set used for evaluation)
│       │
│       └───Traing  (the training set used for training)
│
└───evaluation
    │
    └───nlg_based (scripts used to evaluate the NLG-based metrics)
    │
    └───test_adequacy (scripts used to evaluate test adequacey metrics)   
```

## 1. Dataset
- generated_asserts folder contains the generaetd oracles from the 4 studied Neural Test Oracle Generators (NTOGs).
- injected_asserts folder contains the oracles that injected in each NTOGs for the execution level evaluation (test adequacy metrics).
- mining_atlas folder contains the raw dataset that was used to training and evaluation, and the script to mine those dataset.

Link to download our dataset: [link](https://drive.google.com/drive/folders/13tbuFgytnqR6-BnY8iyrbNMM2I8NJDoc?usp=sharing).

### If you want to run the minig script:

```
$ cd dataset/mining_atlas
$ python get_my_atlas.py
$ python utils.py
```

## 2. Evaluation
### 2.1 NLG-based evaluation metrics
The nlg_based folder contains the script to evaluate the generated oracles for the studied NTOGs with the NLG-based evaluation metrics.
The script contains two types of calculations, 1) the whole test set and 2) project-level execution targets.

```
$ cd evaluation/nlg_based

# for the whole test set.
$ python calculate_nlg.py testset

# for the project-level (to find the correlation between test adequacy metrics).
$ python calculate_nlg.py projects
```

### 2.2 Test Adequacy metrics
The test_adequacy folder contains the script to evaluate the generated oracles that are injected into the 13 projects studied in this paper.
The script contains the following modes of execution:
1. constructing the subproject that only has our target test methods,
2. injects/replaces the oracles generated by the 4 NTOGs
3. running the Pit for calculating the test adequacy metrics.
4. get the Pit results for the actual average score of the test adequacy metrics.

#### The list of NTOG_NAME to pass as argument: ['atlas', 'ir', 'toga', 'chatgpt']

```
$ cd evaluation/test_adequacy

# constructing the subproject
$ python run_pit.py subproject

# injecting the oracles
$ python run_pit.py inject [NTOG_NAME]

# running pit
$ python run_pit.py run_pit [NTOG_NAME]

# get metrics
$ python run_pit.py get_metrics [NTOG_NAME]
```

## 3. To Re-train the NTOGs
1. To train and generate a oracles for a new model, clone each repository of the publicly available replication packages.
```
$ git clone https://gitlab.com/cawatson/atlas---deep-learning-assert-statements.git
$ git clone https://github.com/yh1105/Artifact-of-Assertion-ICSE22.git
$ git clone https://github.com/microsoft/toga.git
```

2. More details can also be found in their original repositories.

    a. ATLAS:
    1. construct a vocabulary on the training data with the script in
    ```
    $ python Source_Code/bin/tools/generate_vocab.py
    ```
    2. train the model with
    ```
    $ bash ./train_test.sh ../dataset/raw/training/ 300000 ../models/raw_model/ assertConfig
    ```

    b. IR:
    1. perform retrieval
    ```
    $ python ./Retrieval/IR.py $input_config $result_path
    ```
    2. perfrom adaptation by heuristic
    ```
    $ python ./Retrieval/IR.py  $result_path New 
    ```
    3. Training the model
    ```
    $ python ./NeuralModel/DataPrepration $input_config $result_path $neural_data_path_train train
    $ python ./NeuralModel/main $neural_data_path_train $neural_result_path train
    $ python ./NeuralModel/DataPrepration.py $input_config $result_path $neural_data_path_evaluate evaluate
    $ python ./NeuralModel/main.py $neural_data_path_evaluate  $neural_result_path evaluate
    ```
    4. Evaluation/generation
    ```
    $ python AdaptionIntegration.py $input_config $result_path $neural_result_path $integration_threshold
    $ python countMultiNewDataSet.py $result_path
    ```
    
    c. TOGA
    1. Initial setup
    ```
    $ cd toga/
    $ git lfs pull
    $ pip install -r requirements.txt
    $ git clone https://gitlab.com/cawatson/atlas---deep-learning-assert-statements.git
    $ export ATLAS_PATH=<path_to_atlas...>/atlas---deep-learning-assert-statements/
    ```
    2. Training
    ```   
    $ cd model/assertions/
    $ bash run_train.sh
    ```
    3. Evaluation
    ```
    $ cd eval/rq2/assertion_inference
    $ bash rq2.sh
    ```

## 4. Appendix
Includes exel sheets that includes the correlation analysis result per project plus the code that we did an manual inspection.